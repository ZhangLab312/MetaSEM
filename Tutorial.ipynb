{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preparation "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use Beeline benchmark to benchmark the performance of MetaSEM.\n",
    "The data preparation process are shown in below.\n",
    "1. Download raw data from https://doi.org/10.5281/zenodo.3378975, which is provided by BEELINE benchmark.\n",
    "2. Download raw data from http://www.grndb.com/browse/result?browsevalue=HNSC_TCGA, which is provided by GRNdb.\n",
    "3. Use the preoprocess code in https://github.com/Murali-group/Beeline/blob/master/generateExpInputs.py to generate dataset.\n",
    "We also provide demo data as shown in ../Data/GRN_inference \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For scientists with very little programming knowledge,we have already packaged the overall model, and the specific steps\n",
    "are as follows: First step, please store the data to be trained into a CSV file in the form of sample-gene and ensure \n",
    "the computing environment meets the requirement.\n",
    "The requirement are:python >=3.6,pytorch >= 1.2.0,scanpy==1.6.0, numpy==1.14.5, pandas==1.0.0, scikit-learn==0.23.2\n",
    "\n",
    "\n",
    "# Run MetaSEM by using following command:\n",
    "Second step Run MetaSEM by using following command:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "For GRN inference:!python main.py --task GRN_inference --setting default --alpha 0.45 --epoch 20 --save_name out\n",
    "For Robust:!python main.py --task Robust --setting default --alpha 0.7 --epoch 20 --save_name out\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If there is no ground truth GRN, please random initialize a pseudo GRN using prior knowledge or randomly (Make sure the \n",
    "genes in pseudo GRN are corresponding to the data's genes).\n",
    "And set the --is_label as None.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "MetaSEM is a deep learning model for gene regulation network reasoning based on the SEM matrix in the meta tool. \n",
     "Thanks to the excellent performance of meta learners on small sample data, our model is still robust on small sample data. \n",
     "In order to maintain this robustness in general tasks, we propose the following strategy for manually arranging models: \n",
     "when the available gene types are less than 400 and the number of samples is not more than 400, the number of feature\n",
     "extractors should be 3 layers, the number of hidden layer neurons should not be more than 128, and the learning rate \n",
     "should be adjusted according to the number of samples in the interval [1e-2,1e-3]. When the available gene types are \n",
     "more than 400 and the number of samples is not less than 400, the number of layers of the feature extractor should not \n",
     "be less than 3, the number of neurons in the hidden layer should be 128, and the learning rate should not be less \n",
     "than 1e-3. For other cases, more detailed parameter search should be carried out according to the task purpose.\n",
     " \n"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}